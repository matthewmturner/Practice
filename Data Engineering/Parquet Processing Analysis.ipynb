{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scope\n",
    "I receive a 140mb csv file daily into a share drive that I would like to use for historical analytics.  I have two problems I am trying to solve for:\n",
    "\n",
    "    1. I want to create a historical parquet dataset as the storage for the historical analytics\n",
    "\n",
    "    2. I want to know what combintation of tools I should use to perform the analytics\n",
    "\n",
    "## Options\n",
    "Problem 1\n",
    "\n",
    "    1. Convert the daily file into its own parquet file and write that.  With that I can then use Dask or Pyarrow.  It's highly unlikely we choose the approach.  The overhead associated we Parquet files such as the page meta data and footers is not optimized for many small files, which this would be (each parquet file would be around 20MB).\n",
    "    \n",
    "    2. Create partioned parquet dataset.  Partitioning strategy to be determined (i.e. weekly/monthly).  In particular partitioning is relevant because parquet does not have an append mode - so I cant simply append to the same parquet file or a file that contains a large number of dates.  As a result reading in the parquet file to memory to append the latest days data can get out of hand quickly (since I am appending all columns).  After a few months I will have an in memory dataframe of 12-16GB which will be very difficult for the PC and server we have to handle.  So a scalable solution is required.\n",
    "\n",
    "Problem 2\n",
    "\n",
    "    1. Pyarrow and pandas.  If were smart with data types and only choosing what columns we want this should suffice.\n",
    "    2. Dask.\n",
    "    3. Combination.  Dask for early steps, reading in and filtering, then convert to pandas when data is a managable size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source:\n",
    "https://www.kaggle.com/benhamner/sf-bay-area-bike-share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:00:23.983605Z",
     "start_time": "2020-03-28T05:00:23.398781Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow import csv\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check total bytes to start is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:00:25.186145Z",
     "start_time": "2020-03-28T05:00:25.177169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa.total_allocated_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:32:23.867434Z",
     "start_time": "2020-03-28T05:32:22.045831Z"
    }
   },
   "outputs": [],
   "source": [
    "data = csv.read_csv(r\"C:\\Users\\matth\\OneDrive\\Data\\Kaggle\\sf-bay-area-bike-share\\status.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:57:29.366636Z",
     "start_time": "2020-03-28T05:57:29.349682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3419272022"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:42:41.624196Z",
     "start_time": "2020-03-28T05:42:29.644080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 71984434 entries, 0 to 71984433\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Dtype \n",
      "---  ------           ----- \n",
      " 0   station_id       int64 \n",
      " 1   bikes_available  int64 \n",
      " 2   docks_available  int64 \n",
      " 3   time             object\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 2.1+ GB\n"
     ]
    }
   ],
   "source": [
    "data.to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:42:11.620667Z",
     "start_time": "2020-03-28T05:41:58.837616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 71984434 entries, 0 to 71984433\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Dtype   \n",
      "---  ------           -----   \n",
      " 0   station_id       category\n",
      " 1   bikes_available  int64   \n",
      " 2   docks_available  int64   \n",
      " 3   time             object  \n",
      "dtypes: category(1), int64(2), object(1)\n",
      "memory usage: 1.7+ GB\n"
     ]
    }
   ],
   "source": [
    "data.to_pandas(categories=['station_id']).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:00:32.140049Z",
     "start_time": "2020-03-28T05:00:32.136082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71984434, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:32:25.418706Z",
     "start_time": "2020-03-28T05:32:25.413741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10300775040"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa.total_allocated_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:32:33.042821Z",
     "start_time": "2020-03-28T05:32:33.020859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3419272022"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T03:30:29.423550Z",
     "start_time": "2020-03-28T03:30:18.509285Z"
    }
   },
   "outputs": [],
   "source": [
    "df = data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T03:30:30.667778Z",
     "start_time": "2020-03-28T03:30:30.654798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 71984434 entries, 0 to 71984433\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Dtype \n",
      "---  ------           ----- \n",
      " 0   station_id       int64 \n",
      " 1   bikes_available  int64 \n",
      " 2   docks_available  int64 \n",
      " 3   time             object\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 2.1+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T03:32:00.816458Z",
     "start_time": "2020-03-28T03:31:59.991658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     1047142\n",
       "7     1047142\n",
       "6     1047142\n",
       "42    1047141\n",
       "34    1047141\n",
       "       ...   \n",
       "31     872235\n",
       "80     872134\n",
       "82     840950\n",
       "83     798868\n",
       "84     731527\n",
       "Name: station_id, Length: 70, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.station_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:05:32.456125Z",
     "start_time": "2020-03-28T05:05:32.439172Z"
    }
   },
   "outputs": [],
   "source": [
    "data_doubled = pa.concat_tables([data,data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T03:34:27.543531Z",
     "start_time": "2020-03-28T03:34:27.540541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143968868, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_doubled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T03:34:54.061672Z",
     "start_time": "2020-03-28T03:34:54.029773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6838544.044"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_doubled.nbytes/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 30 seconds to write a 2GB CSV to a partitioned dataset which compressed to 338MB.  Playing around I tried rewriting the dataset again to see if the write time was the same and it was.  However, I was surprised to see that the data was actually appended which was contrary to my expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:01:13.629309Z",
     "start_time": "2020-03-28T05:01:13.624356Z"
    }
   },
   "outputs": [],
   "source": [
    "partition_path = r\"C:\\Users\\matth\\OneDrive\\Data\\Kaggle\\sf-bay-area-bike-share\\bike-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:01:45.878145Z",
     "start_time": "2020-03-28T05:01:14.462623Z"
    }
   },
   "outputs": [],
   "source": [
    "pq.write_to_dataset(data,\n",
    "                    root_path=partition_path,\n",
    "                    partition_cols=['station_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read that same dataset back into a table takes 6.52 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T03:45:15.211285Z",
     "start_time": "2020-03-28T03:45:04.724613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143968868, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq.read_table(partition_path).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now writing with the double sized dataset the write time goes to 1 minute and 16 seconds.  So performance seems to be non linear with roughly 150% write time for a 100% increase in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:06:57.315760Z",
     "start_time": "2020-03-28T05:05:47.901755Z"
    }
   },
   "outputs": [],
   "source": [
    "pq.write_to_dataset(data_doubled, \n",
    "                    root_path=partition_path,\n",
    "                    partition_cols=['station_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to try reading a specific partition, I will check its shape, and then try just writing that partition many times to see if that also works.  I see there are 5.2mm records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:04:35.568986Z",
     "start_time": "2020-03-28T05:04:35.513569Z"
    }
   },
   "outputs": [],
   "source": [
    "data_23 = pq.ParquetDataset(partition_path, filters=[('station_id', '=', '23')]).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:04:41.101016Z",
     "start_time": "2020-03-28T05:04:41.095033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bikes_available: int64\n",
       "docks_available: int64\n",
       "time: string\n",
       "station_id: dictionary<values=int64, indices=int32, ordered=0>\n",
       "metadata\n",
       "--------\n",
       "{b'pandas': b'{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":'\n",
       "            b' \"bikes_available\", \"field_name\": \"bikes_available\", \"pandas_typ'\n",
       "            b'e\": \"int64\", \"numpy_type\": \"int64\", \"metadata\": null}, {\"name\": '\n",
       "            b'\"docks_available\", \"field_name\": \"docks_available\", \"pandas_type'\n",
       "            b'\": \"int64\", \"numpy_type\": \"int64\", \"metadata\": null}, {\"name\": \"'\n",
       "            b'time\", \"field_name\": \"time\", \"pandas_type\": \"unicode\", \"numpy_ty'\n",
       "            b'pe\": \"object\", \"metadata\": null}], \"creator\": {\"library\": \"pyarr'\n",
       "            b'ow\", \"version\": \"0.16.0\"}, \"pandas_version\": \"1.0.1\"}'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_23.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also confirm the filter worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T04:05:40.218185Z",
     "start_time": "2020-03-28T04:05:39.180960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23    5235705\n",
       "9           0\n",
       "30          0\n",
       "32          0\n",
       "33          0\n",
       "       ...   \n",
       "56          0\n",
       "55          0\n",
       "54          0\n",
       "51          0\n",
       "10          0\n",
       "Name: station_id, Length: 70, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_23.to_pandas().station_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T04:13:28.614409Z",
     "start_time": "2020-03-28T04:13:27.612093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bikes_available</th>\n",
       "      <th>docks_available</th>\n",
       "      <th>time</th>\n",
       "      <th>station_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2013/08/29 12:06:01</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2013/08/29 12:07:01</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2013/08/29 12:08:01</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2013/08/29 12:09:01</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2013/08/29 12:10:01</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bikes_available  docks_available                 time station_id\n",
       "0                8                7  2013/08/29 12:06:01         23\n",
       "1                8                7  2013/08/29 12:07:01         23\n",
       "2                8                7  2013/08/29 12:08:01         23\n",
       "3                8                7  2013/08/29 12:09:01         23\n",
       "4                8                7  2013/08/29 12:10:01         23"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_23.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now write this table back to the dataset 2 more times and I expect if the append operation works as it did on the whole dataset that just that partition will increase by about 10.4mm records.  The write operations themselves were pretty quick coming in around 2.5 seconds each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T04:09:09.017881Z",
     "start_time": "2020-03-28T04:09:06.441644Z"
    }
   },
   "outputs": [],
   "source": [
    "pq.write_to_dataset(data_23,\n",
    "                   root_path=partition_path,\n",
    "                   partition_cols=['station_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T04:10:10.031741Z",
     "start_time": "2020-03-28T04:10:10.026755Z"
    }
   },
   "source": [
    "Now to test if the data was correctly appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T04:10:29.575563Z",
     "start_time": "2020-03-28T04:10:28.795625Z"
    }
   },
   "outputs": [],
   "source": [
    "data_23_2 = pq.ParquetDataset(partition_path, filters=[('station_id', '=', '23')]).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the shape of the table I can see that it did increase by the expected size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T04:10:38.423748Z",
     "start_time": "2020-03-28T04:10:38.419751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15707115, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_23_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another test I will do is restarting the kernel to lose all variables and try appending an additional 2 times.  I'm doing this as a another check as my expectation coming in was that parquet is not appendable and I want to ensure that it will work in real life scenario where the same python session will not be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T04:15:08.510270Z",
     "start_time": "2020-03-28T04:15:07.899734Z"
    }
   },
   "outputs": [],
   "source": [
    "data_23_3 = pq.ParquetDataset(partition_path, filters=[('station_id', '=', '23')]).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T04:15:14.963915Z",
     "start_time": "2020-03-28T04:15:14.955974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15707115, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_23_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T04:15:46.437679Z",
     "start_time": "2020-03-28T04:15:39.364938Z"
    }
   },
   "outputs": [],
   "source": [
    "pq.write_to_dataset(data_23_3,\n",
    "                   root_path=partition_path,\n",
    "                   partition_cols=['station_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After restarting the kernel, rewriting the data, and reading in the partition I see the expected number of rows.  It appears this solution will work to append data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T04:49:44.535953Z",
     "start_time": "2020-03-28T04:49:43.432195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31414230, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq.ParquetDataset(partition_path, filters=[('station_id', '=', '23')]).read().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T04:51:10.239129Z",
     "start_time": "2020-03-28T04:51:03.444840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23]\n",
       "Categories (1, int64): [23]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq.ParquetDataset(partition_path, filters=[('station_id', '=', '23')]).read().to_pandas().station_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After digging into the directory I see that all the additional writes that I did added individual parquet files.  Which is actually very inefficient as highlighted before.  So an alternative solution will have to be investigated.  Everything above can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write_table gives the ability to overwrite a single file which may come in handy.  A possible approach to this could be to pick manually partition on something like a combination of year and date and simply overwrite the relevant partition daily.  This would limit each parquet file size to around 400-500MB (in the real data) which translates to around 4-5GB in pandas which is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:17:09.682160Z",
     "start_time": "2020-03-28T05:17:03.558867Z"
    }
   },
   "outputs": [],
   "source": [
    "pq.write_table(data, \"bikes.parquet.snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:11:46.133500Z",
     "start_time": "2020-03-28T05:11:46.111334Z"
    }
   },
   "outputs": [],
   "source": [
    "data_quad = pa.concat_tables([data,data,data,data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T05:12:29.022727Z",
     "start_time": "2020-03-28T05:12:06.296031Z"
    }
   },
   "outputs": [],
   "source": [
    "pq.write_table(data_quad, \"bikes.parquet.snappy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('learning': conda)",
   "language": "python",
   "name": "python37664bitlearningconda19657ba668a546319d94c84a2735f47c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
